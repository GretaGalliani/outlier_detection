# NUMBER OF OUTLIERS AND CLUSTERS
print(paste0('Number of outliers: ', length(out_i)))
print(paste0('Number of clusters: ', length(cl_i)))
out_names
factor(min_vi$cl)
table(min_vi$cl)
clust_map = min_vi$cl
clust_map[which(clust_map %in% out_i)] = 0
uniq = 1:length(unique(clust_map))
levels = as.numeric(levels(factor(clust_map)))
for (i in uniq){
if(levels[i]>uniq[i]-1){
clust_map[which(clust_map==levels[i])]=i-1
}
}
print("Outliers: ")
print(out_names)
count = 1
for (i in 1:length(cl_i)){
print(paste0("Cluster number ", count))
print(country[which(min_vi$cl == cl_i[i])])
count = count+1
}
# Map plot
map_data = data.frame(country = country, cluster = clust_map)
map = joinCountryData2Map(map_data, joinCode = "NAME", nameJoinColumn = "country",
nameCountryColumn = "country", suggestForFailedCodes = TRUE,
mapResolution = "coarse", projection = NA, verbose = FALSE)
#jpeg("map_Q07_P005_CovDiv_08.jpg", width = 700, height = 700)
x11()
mapCountryData(map, nameColumnToPlot = 'cluster', catMethod = 'categorical',
colourPalette = 'rainbow', mapTitle = 'Country clusters')
library(MASS)
library(RColorBrewer)
library(robustbase)
library(TSstudio)
doge = read.csv('DOGE-USD.csv')
date = as.Date.character(doge$Date)
doge$LogReturn = rep(0, dim(doge)[1])
for (i in 2:dim(doge)[1]){
doge$LogReturn[i] = log(doge$Adj.Close[i]/doge$Adj.Close[i-1])*100
}
data = as.matrix(doge$LogReturn)
data=data[1:365,]
data= scale(data)
n = dim(data)[1]
d = dim(data)[2]
# Initialization of the parameters for the priors
Q_param = list()
P_param = list()
# Contaminated component
Q_param$k_0 = 0.9
Q_param$mu_0 = mean(data)
Q_param$nu_0 = d+5
Q_param$lambda_0 = var(data)/6
# Contaminant diffuse component
P_param$k_0 = 0.01
P_param$mu_0 = mean(data)
P_param$nu_0 = d+5
P_param$lambda_0 = var(data)/6
# Initialization of the parameters for the Pitman-Yor and initial partition
S_init = rep(1, n)
beta_init <- 0.5
sigma_init <- 0.2
theta_init <- 0.2
beta_param = list()
sigma_param = list()
theta_param = list()
beta_param$a = 1
beta_param$b = 1
sigma_param$a = 1
sigma_param$b = 1
theta_param$a = 2
theta_param$b = 1
xi_mu <- list()
xi_cov <- list()
init_mu <- mean(data)
init_var <- var(data)
#put iter back to 2500
result <- algorithm(data, S_init, sigma_init, theta_init, beta_init, beta_param, sigma_param, theta_param, xi_mu, xi_cov, Q_param, P_param, 2500, 100, 1)
source("main.R")
#put iter back to 2500
result <- algorithm(data, S_init, sigma_init, theta_init, beta_init, beta_param, sigma_param, theta_param, xi_mu, xi_cov, Q_param, P_param, 2500, 100, 1)
source("main.R")
#put iter back to 2500
result <- algorithm(data, S_init, sigma_init, theta_init, beta_init, beta_param, sigma_param, theta_param, xi_mu, xi_cov, Q_param, P_param, 2500, 100, 1)
init_mu <- mean(data)
init_var <- var(data)
for (i in 1:dim(data)[1]){
xi_mu <- append(xi_mu, list(init_mu))
xi_cov <- append(xi_cov, list(init_var))
}
library(MASS)
library(RColorBrewer)
library(robustbase)
library(TSstudio)
# Import dataset DOGE-USD
doge = read.csv('DOGE-USD.csv')
date = as.Date.character(doge$Date)
doge$LogReturn = rep(0, dim(doge)[1])
for (i in 2:dim(doge)[1]){
doge$LogReturn[i] = log(doge$Adj.Close[i]/doge$Adj.Close[i-1])*100
}
#set.seed(42)
#rows <- sample(nrow(doge))
#doge <- doge[rows, ]
#doge$Adj.Close = NULL
#ts_plot(data.frame(date = date, LogReturn=doge$LogReturn))
data = as.matrix(doge$LogReturn)
data=data[1:365,]
data= scale(data)
n = dim(data)[1]
d = dim(data)[2]
# Initialization of the parameters for the priors
Q_param = list()
P_param = list()
# Contaminated component
Q_param$k_0 = 0.9
Q_param$mu_0 = mean(data)
Q_param$nu_0 = d+5
Q_param$lambda_0 = var(data)/6
# Contaminant diffuse component
P_param$k_0 = 0.01
P_param$mu_0 = mean(data)
P_param$nu_0 = d+5
P_param$lambda_0 = var(data)/6
# Initialization of the parameters for the Pitman-Yor and initial partition
S_init = rep(1, n)
beta_init <- 0.5
sigma_init <- 0.2
theta_init <- 0.2
beta_param = list()
sigma_param = list()
theta_param = list()
beta_param$a = 1
beta_param$b = 1
sigma_param$a = 1
sigma_param$b = 1
theta_param$a = 2
theta_param$b = 1
xi_mu <- list()
xi_cov <- list()
init_mu <- mean(data)
init_var <- var(data)
for (i in 1:dim(data)[1]){
xi_mu <- append(xi_mu, list(init_mu))
xi_cov <- append(xi_cov, list(init_var))
}
library(MASS)
library(RColorBrewer)
library(robustbase)
library(TSstudio)
# Import dataset DOGE-USD
doge = read.csv('DOGE-USD.csv')
date = as.Date.character(doge$Date)
doge$LogReturn = rep(0, dim(doge)[1])
for (i in 2:dim(doge)[1]){
doge$LogReturn[i] = log(doge$Adj.Close[i]/doge$Adj.Close[i-1])*100
}
#set.seed(42)
#rows <- sample(nrow(doge))
#doge <- doge[rows, ]
#doge$Adj.Close = NULL
#ts_plot(data.frame(date = date, LogReturn=doge$LogReturn))
data = as.matrix(doge$LogReturn)
data=data[1:365,]
data= scale(data)
n = dim(data)[1]
d = dim(data)[2]
# Initialization of the parameters for the priors
Q_param = list()
P_param = list()
# Contaminated component
Q_param$k_0 = 0.9
Q_param$mu_0 = mean(data)
Q_param$nu_0 = d+5
Q_param$lambda_0 = var(data)/6
# Contaminant diffuse component
P_param$k_0 = 0.01
P_param$mu_0 = mean(data)
P_param$nu_0 = d+5
P_param$lambda_0 = var(data)/6
# Initialization of the parameters for the Pitman-Yor and initial partition
S_init = rep(1, n)
beta_init <- 0.5
sigma_init <- 0.2
theta_init <- 0.2
beta_param = list()
sigma_param = list()
theta_param = list()
beta_param$a = 1
beta_param$b = 1
sigma_param$a = 1
sigma_param$b = 1
theta_param$a = 2
theta_param$b = 1
xi_mu <- list()
xi_cov <- list()
init_mu <- mean(data)
init_var <- var(data)
for (i in 1:dim(data)[1]){
xi_mu <- append(xi_mu, list(init_mu))
xi_cov <- append(xi_cov, list(init_var))
}
#put iter back to 2500
result <- algorithm(data, S_init, sigma_init, theta_init, beta_init, beta_param, sigma_param, theta_param, xi_mu, xi_cov, Q_param, P_param, 2500, 100, 1)
source("main.R")
#put iter back to 2500
result <- algorithm(data, S_init, sigma_init, theta_init, beta_init, beta_param, sigma_param, theta_param, xi_mu, xi_cov, Q_param, P_param, 2500, 100, 1)
save(result, file='per-lorenzo.RData')
plot(result$sigma)
mean(result$sigma)
result$acc_sigma
result$acc_beta
result$acc_theta
set.seed(04021997)
library(MASS)
library(RColorBrewer)
#### SAMPLING FROM THE DENSITY ####
d = 2 #dimension
mean_a = rep(-3,d)#vector of means
sigma_b = diag(d) #sigma
m = 90 #m number of samples (outliers excluded) {90, 240}
m1 = rbinom(1, size=m, prob = 0.5) #number of samples coming from the first gaussian
m2 = m-m1 # from the second one
val1 = mvrnorm (m1,mu = mean_a, Sigma = sigma_b) #samples from first multivariate function
val2 = mvrnorm (m2,mu = -mean_a, Sigma = sigma_b) #samples from second multivariate function
allval = rbind(val1,val2) #combine
# CONTAMINATED MEASURE
s=10 #number of outliers
i=0
c=1
while(i<s){ #cycle to find s outliers
value=mvrnorm(1,mu= rep(0,d),Sigma= 3^2*diag(d)) #sampling from a multivariate normal distribution
module = norm(as.matrix(value), type="2")
chi=qchisq(0.9, df = d)
# if(module^2>3*sqrt(chi))
if(module^2>9*chi) #If we are sampling from the over-disperse truncated Gaussian distribution
{
i=i+1
value = c*value #C has the role to shrink or expand the nuisance observations towards the origin
allval = rbind(allval,value)
}
}
# Constructing the dataset
rownames(allval)=NULL
allval
data <- allval
# Plotting the data with the original groups
pal = brewer.pal(n = 9, name = "Set1")
col_real = c(rep(pal[1], m1), rep(pal[3], m-m1), rep(pal[2],s))
pc = c(rep(16,m), rep(17,s))
pairs(data, col = col_real, pch = pc, main = "Real data")
Q_param$k_0 = 1 #{1, 0.5}
Q_param$mu_0 = c(0,0)
Q_param = list()
P_param = list()
d = dim(data)[2]
Q_param$k_0 = 1 #{1, 0.5}
Q_param$mu_0 = c(0,0)
Q_param$nu_0 = d + 3 # it must be > (p-1)
Q_param$lambda_0 = diag(diag(cov(data)))
P_param$k_0 = 0.25 #{0.25,0.5}
P_param$mu_0 = c(0,0)
P_param$nu_0 = d + 3 # it must be > (p-1)
P_param$lambda_0 = diag(diag(cov(data)))
n = dim(data)[1]
S_init = rep(1,n)
beta_init <- 0.5
sigma_init <- 0.5
theta_init <- 1
beta_param = list()
sigma_param = list()
theta_param = list()
beta_param$a = 1
beta_param$b = 1
sigma_param$a = 1
sigma_param$b = 1
#Gamma con picco in 1
theta_param$a = 2
theta_param$b = 1
xi_mu <- list()
xi_cov <- list()
init_mu <- colMeans(data)
init_var <- cov(data)
for (i in 1:n){
xi_mu <- append(xi_mu, list(init_mu))
xi_cov <- append(xi_cov, list(init_var))
}
result <- algorithm(data, S_init, sigma_init, theta_init, beta_init, beta_param, sigma_param, theta_param, xi_mu, xi_cov, Q_param, P_param, 2000, 500, 1)
# RUNNING THE ALGORITHM ####
source("main.R")
result <- algorithm(data, S_init, sigma_init, theta_init, beta_init, beta_param, sigma_param, theta_param, xi_mu, xi_cov, Q_param, P_param, 2000, 500, 1)
# RUNNING THE ALGORITHM ####
source("main.R")
# RUNNING THE ALGORITHM ####
source("main.R")
result <- algorithm(data, S_init, sigma_init, theta_init, beta_init, beta_param, sigma_param, theta_param, xi_mu, xi_cov, Q_param, P_param, 2000, 500, 1)
x11()
par(mfrow=c(1,3))
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
plot(result$sigma,xlab="iteration",ylab=expression(sigma), type = 'l')
plot(result$theta,xlab="iteration",ylab=expression(theta), type = 'l')
plot(result$beta,xlab="iteration",ylab=expression(beta), type = 'l')
max <- c()
for (i in 1:10){
max <- c(max, max(result$S[i,]))
}
meand(max) # mean number of clusters by the algorithm
# NUMBER OF SINGLETONS
source("algorithm_v1/auxiliary_functions.R")
n_singletons <- c()
for (i in 1:dim(result$S)[1]){
n_singletons <- c(n_singletons, m1(result$S[i,]))
}
## LOSS FUNCTION ####
#Binder loss function
library(mcclust)
aux = result$S
# These functions needs to have indexes of the groups >=1
for (i in 1:dim(aux)[1]){
for (j in 1:dim(aux)[2]){
if (aux[i,j]==0){
aux[i,j] = max(aux[i,]) + 1
}
}
}
psm <- comp.psm(aux)
# finds the clustering that minimizes the posterior expectation of Binders loss function
min_bind <-  minbinder(psm, cls.draw = NULL, method = c("avg", "comp", "draws",
"laugreen","all"), max.k = NULL, include.lg = FALSE,
start.cl = NULL, tol = 0.001)
# best cluster according binder loss
# NOTE: We report only the graph for CPY1, n=100, c=1
tab_bind <- table(min_bind$cl)
pal = brewer.pal(n = 9, name = "Set1")
col_bind = c(rep(pal[1], tab_bind[[1]]),rep(pal[3], tab_bind[[7]]), rep(pal[4], tab_bind[[8]]),
rep(pal[5], tab_bind[[10]]), rep(pal[2],18))
pc = c(rep(16,82), rep(17,18))
# best cluster according binder loss
# NOTE: We report only the graph for CPY1, n=100, c=1
tab_bind <- table(min_bind$cl)
tab_bind
pal = brewer.pal(n = 9, name = "Set1")
col_bind = c(rep(pal[1], tab_bind[[1]]),rep(pal[3], tab_bind[[7]]), rep(pal[4], tab_bind[[8]]),
rep(pal[5], tab_bind[[10]]), rep(pal[2],18))
pc = c(rep(16,82), rep(17,18))
#Plot real data
col_real <- c(rep(pal[1],43),rep(pal[3],47),rep(pal[2],s))
plot(data,col=col_real,pch=pc)
pal = brewer.pal(n = 8, name = "Set1")
col_bind = c(rep(pal[1], tab_bind[[1]]),rep(pal[3], tab_bind[[7]]), rep(pal[4], tab_bind[[8]]),
rep(pal[5], tab_bind[[10]]), rep(pal[2],18))
pc = c(rep(16,82), rep(17,18))
#Plot real data
col_real <- c(rep(pal[1],43),rep(pal[3],47),rep(pal[2],s))
pal = brewer.pal(n = 9, name = "Set1")
col_bind = c(rep(pal[1], tab_bind[[1]]),rep(pal[3], tab_bind[[7]]), rep(pal[4], tab_bind[[8]]),
rep(pal[5], tab_bind[[10]]), rep(pal[2],18))
pc = c(rep(16,82), rep(17,18))
#Plot real data
col_real <- c(rep(pal[1],43),rep(pal[3],47),rep(pal[5],s))
plot(data,col=col_real,pch=pc)
#Plot real data
col_real <- c(rep(pal[1],43),rep(pal[3],47),rep(pal[7],s))
plot(data,col=col_real,pch=pc)
#Plot real data
col_real <- c(rep(pal[1],43),rep(pal[3],47),rep(pal[2],s))
plot(data,col=col_real,pch=pc)
#Plot real data
col_real <- c(rep(pal[1],43),rep(pal[3],47),rep(pal[4],s))
plot(data,col=col_real,pch=pc)
#Plot real data
col_real <- c(rep(pal[1],43),rep(pal[3],47),rep(pal[5],s))
plot(data,col=col_real,pch=pc)
#Plot real data
col_real <- c(rep(pal[1],43),rep(pal[3],47),rep(pal[6],s))
plot(data,col=col_real,pch=pc)
#Plot real data
col_real <- c(rep(pal[1],43),rep(pal[3],47),rep(pal[9],s))
plot(data,col=col_real,pch=pc)
# IMPLEMENTING MIN VARIATION OF INFORMATION
#devtools::install_github("sarawade/mcclust.ext")
library(mcclust.ext)
# finds the clustering that minimizes  the lower bound to the posterior expected Variation of Information from Jensen's Inequality
min_vi <- minVI(psm, cls.draw=NULL, method=c("avg","comp","draws","greedy","all"),
max.k=NULL, include.greedy=FALSE, start.cl=NULL, maxiter=NULL,
l=NULL, suppress.comment=TRUE)
# best cluster according to binder loss
# NOTE: We report only the graph for CPY1, n=100, c=1
tab_vi <- table(min_vi$cl)
tab_vi
pal = brewer.pal(n = 9, name = "Set1")
col_bind = c(rep(pal[1], tab_vi[[1]]),rep(pal[3], tab_vi[[2]]),
rep(pal[2],9))
pc = c(rep(16,91), rep(17,9))
plot(data, col = col_bind, pch = pc)
#Plot real data
col_real <- c(rep(pal[1],43),rep(pal[3],47),rep("black",s))
plot(data,col=col_real,pch=pc)
# best cluster according binder loss
# NOTE: We report only the graph for CPY1, n=100, c=1
tab_bind <- table(min_bind$cl)
data
#Plot real data
col_real <- c(rep(pal[1],43),rep(pal[3],47),rep(pal[9],s))
plot(data,col=col_real,pch=pc)
# Plotting the data with the original groups
pal = brewer.pal(n = 9, name = "Set1")
col_real = c(rep(pal[1], m1), rep(pal[3], m-m1), rep(pal[2],s))
rep(pal[2],s)
rep(pal[3], m-m1)
col_real = c(rep(pal[1], 43), rep(pal[3], 47), rep(pal[2],s))
pc = c(rep(16,m), rep(17,s))
col_real = c(rep(pal[1], 44), rep(pal[3], 47), rep(pal[2],s))
pc = c(rep(16,m), rep(17,s))
col_real = c(rep(pal[1], 44), rep(pal[3], 46), rep(pal[2],s))
pc = c(rep(16,m), rep(17,s))
pairs(data, col = col_real, pch = pc, main = "Real data")
# Plotting the data with the original groups
pal = brewer.pal(n = 9, name = "Set1")
col_real = c(rep(pal[1], 43), rep(pal[3], 47), rep(pal[2],s))
pc = c(rep(16,m), rep(17,s))
plot(data, col = col_real, pch = pc, main = "Real data")
Q_param = list()
col_real = c(rep(pal[1], 43), rep(pal[3], 47), rep("black",s))
pc = c(rep(16,m), rep(17,s))
plot(data, col = col_real, pch = pc, main = "Real data")
# best cluster according binder loss
# NOTE: We report only the graph for CPY1, n=100, c=1
tab_bind <- table(min_bind$cl)
pal = brewer.pal(n = 9, name = "Set1")
col_bind = c(rep(pal[1], tab_bind[[1]]),rep(pal[3], tab_bind[[7]]), rep(pal[4], tab_bind[[8]]),
rep(pal[5], tab_bind[[10]]), rep(pal[2],18))
tab_bind
pc = c(rep(16,81), rep(17,19))
#Plot real data
col_real <- c(rep(pal[1],43),rep(pal[3],47),rep(pal[9],s))
plot(data,col=col_real,pch=pc)
col_bind = c(rep(pal[1], tab_bind[[1]]),rep(pal[3], tab_bind[[7]]), rep(pal[4], tab_bind[[8]]),
rep(pal[5], tab_bind[[10]]), rep("black",18))
pc = c(rep(16,81), rep(17,19))
#Plot clustering Binder loss
plot(data,col=col_bind,pch=pc)
# IMPLEMENTING MIN VARIATION OF INFORMATION
#devtools::install_github("sarawade/mcclust.ext")
library(mcclust.ext)
# finds the clustering that minimizes  the lower bound to the posterior expected Variation of Information from Jensen's Inequality
min_vi <- minVI(psm, cls.draw=NULL, method=c("avg","comp","draws","greedy","all"),
max.k=NULL, include.greedy=FALSE, start.cl=NULL, maxiter=NULL,
l=NULL, suppress.comment=TRUE)
# best cluster according to binder loss
# NOTE: We report only the graph for CPY1, n=100, c=1
tab_vi <- table(min_vi$cl)
tab_vi
pal = brewer.pal(n = 9, name = "Set1")
col_bind = c(rep(pal[1], tab_vi[[1]]),rep(pal[3], tab_vi[[2]]),
rep("black",9))
pc = c(rep(16,91), rep(17,9))
plot(data, col = col_bind, pch = pc)
# best cluster according binder loss
# NOTE: We report only the graph for CPY1, n=100, c=1
tab_bind <- table(min_bind$cl)
pal = brewer.pal(n = 9, name = "Set1")
col_bind = c(rep(pal[1], tab_bind[[1]]),rep(pal[3], tab_bind[[7]]), rep(pal[4], tab_bind[[8]]),
rep(pal[5], tab_bind[[10]]), rep("black",18))
tab_bind
col_bind = c(rep(pal[1], tab_bind[[1]]),rep(pal[3], tab_bind[[7]]), rep(pal[4], tab_bind[[8]]),
rep(pal[5], tab_bind[[10]]), rep("black",19))
pc = c(rep(16,81), rep(17,19))
#Plot real data
col_real <- c(rep(pal[1],43),rep(pal[3],47),rep(pal[9],s))
plot(data,col=col_real,pch=pc)
#Plot clustering Binder loss
plot(data,col=col_bind,pch=pc, main = "Real data")
col_bind = c(rep(pal[1], tab_bind[[1]]),rep(pal[7], tab_bind[[7]]), rep(pal[4], tab_bind[[8]]),
rep(pal[5], tab_bind[[10]]), rep("black",19))
pc = c(rep(16,81), rep(17,19))
#Plot clustering Binder loss
plot(data,col=col_bind,pch=pc, main = "Real data")
tab_bind
#Plot clustering Binder loss
plot(data,col=col_bind,pch=pc, main = "Real data")
col_bind = c(rep(pal[1], tab_bind[[1]]),rep(pal[3], tab_bind[[7]]), rep(pal[4], tab_bind[[8]]),
rep(pal[5], tab_bind[[10]]), rep("black",19))
#Plot clustering Binder loss
plot(data,col=col_bind,pch=pc, main = "Real data")
col_bind = c(rep(pal[1], tab_bind[[1]]),rep(pal[3], tab_bind[[7]]), rep(pal[5], tab_bind[[8]]),
rep(pal[5], tab_bind[[10]]), rep("black",19))
pc = c(rep(16,81), rep(17,19))
#Plot clustering Binder loss
plot(data,col=col_bind,pch=pc, main = "Real data")
col_bind = c(rep(pal[1], tab_bind[[1]]),rep(pal[3], tab_bind[[7]]), rep(pal[7], tab_bind[[8]]),
rep(pal[5], tab_bind[[10]]), rep("black",19))
pc = c(rep(16,81), rep(17,19))
#Plot clustering Binder loss
plot(data,col=col_bind,pch=pc, main = "Real data")
col_bind = c(rep(pal[1], tab_bind[[1]]),rep(pal[3], tab_bind[[7]]), rep(pal[7], tab_bind[[8]]),
rep(pal[8], tab_bind[[10]]), rep("black",19))
pc = c(rep(16,81), rep(17,19))
#Plot clustering Binder loss
plot(data,col=col_bind,pch=pc, main = "Real data")
col_bind = c(rep(pal[1], tab_bind[[1]]),rep(pal[3], tab_bind[[7]]), rep(pal[9], tab_bind[[8]]),
rep(pal[8], tab_bind[[10]]), rep("black",19))
pc = c(rep(16,81), rep(17,19))
#Plot clustering Binder loss
plot(data,col=col_bind,pch=pc, main = "Real data")
col_bind = c(rep(pal[1], tab_bind[[1]]),rep(pal[3], tab_bind[[7]]), rep("blue", tab_bind[[8]]),
rep(pal[8], tab_bind[[10]]), rep("black",19))
pc = c(rep(16,81), rep(17,19))
#Plot clustering Binder loss
plot(data,col=col_bind,pch=pc, main = "Real data")
